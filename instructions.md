# Instructions: Feature Importance Experiment

## Overview
This experiment analyzes how SLIM's tree depth (`max_depth`) affects feature selection frequency and sparsity.

## 1. Launching the Experiment

Use the provided launcher script `launch_features.sh` to submit jobs to the cluster. This script handles the SLURM array submission in batches.

### Prerequisites
*   Ensure you are on the login node.
*   Ensure `config/task_list_features.csv` exists (generated by `scripts/generate_feature_exp_tasks.py`).

### Commands
To launch **all** tasks (covering all datasets, depths, and seeds):
```bash
./launch_features.sh --all
```

To launch a specific subset (e.g., first 50 tasks for testing):
```bash
./launch_features.sh --count 50
```

To launch a specific range (e.g., tasks 100 to 149):
```bash
./launch_features.sh --from 100 --to 149
```

**Note**: The launcher submits jobs in batches of 50 to avoid overloading the scheduler.

## 2. Monitoring
*   Logs are saved to `logs/slurm/slim_feat_*.out`.
*   Feature importance results are saved to `logs/feature_importance/<dataset>/depth_<depth>/features_seed_<seed>.csv`.

## 3. Analysis

Once all jobs have completed, run the analysis script to generate visualizations.

```bash
python scripts/analyze_feature_histograms.py --results_dir logs/feature_importance --output_dir analysis_results
```

### Outputs (`analysis_results/`)
1.  **`{dataset}_total_occurrences.png`**: Total count of feature appearances across all runs.
2.  **`{dataset}_selection_prob.png`**: Probability of a feature being selected (fraction of runs where it appears). **This is the primary metric for "consistency".**
3.  **`{dataset}_sparsity.png`**: Average number of unique features used per model vs. `max_depth`.

## 4. Verification

To verify that the feature counting is working correctly, you can check the CSV files directly.
Example format:
```csv
dataset,max_depth,seed,feature_index,count
eeg,5,42,0,0.0
eeg,5,42,1,1.0  <-- Feature 1 used 1 time in this run
...
```
Distinct plots for *total occurrences* and *run-wise selection probability* ensure you can distinguish between "a feature used many times in one tree" vs "a feature used consistently across many trees".
